{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bba70245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkFirst\") \\\n",
    "      .getOrCreate() \n",
    "\n",
    "data_schema = [\n",
    "               StructField('VendorID', IntegerType(), True),\n",
    "               StructField('tpep_pickup_datetime', DateType(), True),\n",
    "               StructField('tpep_dropoff_datetime', DateType(), True),\n",
    "               StructField('passenger_count', IntegerType(), True),\n",
    "               StructField('trip_distance', DoubleType(), True),\n",
    "               StructField('RatecodeID', IntegerType(), True),\n",
    "               StructField('store_and_fwd_flag', StringType(), True),\n",
    "               StructField('PULocationID', IntegerType(), True),\n",
    "               StructField('DOLocationID', IntegerType(), True),\n",
    "               StructField('payment_type', IntegerType(), True),\n",
    "               StructField('fare_amount', DoubleType(), True),\n",
    "               StructField('extra', DoubleType(), True),\n",
    "               StructField('mta_tax', DoubleType(), True),\n",
    "               StructField('tip_amount', DoubleType(), True),\n",
    "               StructField('tolls_amount', DoubleType(), True),\n",
    "               StructField('improvement_surcharge', DoubleType(), True),\n",
    "               StructField('total_amount', DoubleType(), True),\n",
    "               StructField('congestion_surcharge', DoubleType(), True),\n",
    "            ]\n",
    "\n",
    "final_struc = StructType(fields = data_schema)\n",
    "\n",
    "#чтение файла и добавления в объект DataFrame\n",
    "df = spark.read.csv('yellow_tripdata_2020-01.csv', sep=\",\", header=True, schema=final_struc)\n",
    "\n",
    "#очищаем данные\n",
    "df=df.dropna(subset=['passenger_count'])\n",
    "df=df.filter(col(\"trip_distance\") > 0)\n",
    "df=df.filter(col(\"fare_amount\") > 0) \n",
    "df=df.filter(col(\"tip_amount\") > 0)\n",
    "\n",
    "#убираем время из даты\n",
    "df = df.withColumn('tpep_dropoff_datetime', date_format(df['tpep_dropoff_datetime'], 'yyyy-MM-dd'))\n",
    "df = df.withColumn('tpep_dropoff_datetime', to_date(df['tpep_dropoff_datetime'], 'yyyy-MM-dd'))\n",
    "\n",
    "#создаем временную таблицу\n",
    "temp_table_name = 'stage'\n",
    "df.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29832b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#вытаскиваем нужные для расчетов данные и создаем группы по кол-ву пассажиров\n",
    "sql=\"\"\"\n",
    "select \n",
    "     tpep_dropoff_datetime,\n",
    "     passenger_count,\n",
    "     case \n",
    "        when passenger_count = 0 then '0'\n",
    "        when passenger_count = 1 then '1'\n",
    "        when passenger_count = 2 then '2'\n",
    "        when passenger_count = 3 then '3'\n",
    "        else '4_plus'\n",
    "    end as passenger_group,\n",
    "    trip_distance,  \n",
    "    tip_amount, \n",
    "    total_amount\n",
    "from stage\n",
    "\"\"\"\n",
    "#записываем в таблицу pas_group\n",
    "df = spark.sql(sql)\n",
    "df.createOrReplaceTempView('pas_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03dde06f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#рассчитываем процент поездок по кол-ву человек в машине\n",
    "sql=\"\"\"\n",
    "select\n",
    "    tpep_dropoff_datetime,\n",
    "    passenger_group,\n",
    "    case when passenger_group like '0' then percentage else '-' end as percentage_zero,\n",
    "    case when passenger_group like '1' then percentage else '-' end as percentage_1p,\n",
    "    case when passenger_group like '2' then percentage else '-' end as percentage_2p,\n",
    "    case when passenger_group like '3' then percentage else '-' end as percentage_3p,\n",
    "    case when passenger_group like '4_plus' then percentage else '-' end as percentage_4p_plus\n",
    "from(\n",
    "select \n",
    "    tpep_dropoff_datetime,\n",
    "    passenger_group,\n",
    "    round(100*count(passenger_count) / sum(count(passenger_count)) over (partition by tpep_dropoff_datetime),2) AS percentage\n",
    "from pas_group\n",
    "group by tpep_dropoff_datetime, passenger_group\n",
    ")\n",
    "\"\"\"\n",
    "#записываем в таблицу pas_perc\n",
    "df = spark.sql(sql)\n",
    "df.createOrReplaceTempView('pas_perc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82f71ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#рассчитываем самую дорогую и самую дешевую поездкой для каждой группы\n",
    "sql=\"\"\"\n",
    "select distinct\n",
    "    tpep_dropoff_datetime,\n",
    "    passenger_group,\n",
    "    max(total_amount) over (partition by tpep_dropoff_datetime, passenger_group) AS max,\n",
    "    min(total_amount) over (partition by tpep_dropoff_datetime, passenger_group) AS min\n",
    "from pas_group\n",
    "\"\"\"\n",
    "#записываем в таблицу total_min_max\n",
    "df = spark.sql(sql)\n",
    "df.createOrReplaceTempView('total_min_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "049af98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#составляем итоговую таблицу\n",
    "sql=\"\"\"\n",
    "select \n",
    "    tpep_dropoff_datetime AS date,  \n",
    "    percentage_zero, \n",
    "    percentage_1p, \n",
    "    percentage_2p, \n",
    "    percentage_3p, \n",
    "    percentage_4p_plus,\n",
    "    min,max\n",
    "from pas_perc join total_min_max using(tpep_dropoff_datetime, passenger_group)\n",
    "order by 1;\n",
    "\"\"\"\n",
    "#записываем в таблицу result\n",
    "df = spark.sql(sql)\n",
    "df.createOrReplaceTempView('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4778670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#записываем результат в файл с расширением .parquet\n",
    "df.toPandas().to_parquet('taxi_result.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f800366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
